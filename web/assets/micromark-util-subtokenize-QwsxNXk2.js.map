{"version":3,"file":"micromark-util-subtokenize-QwsxNXk2.js","sources":["../../node_modules/micromark-util-subtokenize/index.js"],"sourcesContent":["/**\n * @typedef {import('micromark-util-types').Chunk} Chunk\n * @typedef {import('micromark-util-types').Event} Event\n * @typedef {import('micromark-util-types').Token} Token\n */\n\nimport {splice} from 'micromark-util-chunked'\n/**\n * Tokenize subcontent.\n *\n * @param {Array<Event>} events\n *   List of events.\n * @returns {boolean}\n *   Whether subtokens were found.\n */\nexport function subtokenize(events) {\n  /** @type {Record<string, number>} */\n  const jumps = {}\n  let index = -1\n  /** @type {Event} */\n  let event\n  /** @type {number | undefined} */\n  let lineIndex\n  /** @type {number} */\n  let otherIndex\n  /** @type {Event} */\n  let otherEvent\n  /** @type {Array<Event>} */\n  let parameters\n  /** @type {Array<Event>} */\n  let subevents\n  /** @type {boolean | undefined} */\n  let more\n  while (++index < events.length) {\n    while (index in jumps) {\n      index = jumps[index]\n    }\n    event = events[index]\n\n    // Add a hook for the GFM tasklist extension, which needs to know if text\n    // is in the first content of a list item.\n    if (\n      index &&\n      event[1].type === 'chunkFlow' &&\n      events[index - 1][1].type === 'listItemPrefix'\n    ) {\n      subevents = event[1]._tokenizer.events\n      otherIndex = 0\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'lineEndingBlank'\n      ) {\n        otherIndex += 2\n      }\n      if (\n        otherIndex < subevents.length &&\n        subevents[otherIndex][1].type === 'content'\n      ) {\n        while (++otherIndex < subevents.length) {\n          if (subevents[otherIndex][1].type === 'content') {\n            break\n          }\n          if (subevents[otherIndex][1].type === 'chunkText') {\n            subevents[otherIndex][1]._isInFirstContentOfListItem = true\n            otherIndex++\n          }\n        }\n      }\n    }\n\n    // Enter.\n    if (event[0] === 'enter') {\n      if (event[1].contentType) {\n        Object.assign(jumps, subcontent(events, index))\n        index = jumps[index]\n        more = true\n      }\n    }\n    // Exit.\n    else if (event[1]._container) {\n      otherIndex = index\n      lineIndex = undefined\n      while (otherIndex--) {\n        otherEvent = events[otherIndex]\n        if (\n          otherEvent[1].type === 'lineEnding' ||\n          otherEvent[1].type === 'lineEndingBlank'\n        ) {\n          if (otherEvent[0] === 'enter') {\n            if (lineIndex) {\n              events[lineIndex][1].type = 'lineEndingBlank'\n            }\n            otherEvent[1].type = 'lineEnding'\n            lineIndex = otherIndex\n          }\n        } else {\n          break\n        }\n      }\n      if (lineIndex) {\n        // Fix position.\n        event[1].end = Object.assign({}, events[lineIndex][1].start)\n\n        // Switch container exit w/ line endings.\n        parameters = events.slice(lineIndex, index)\n        parameters.unshift(event)\n        splice(events, lineIndex, index - lineIndex + 1, parameters)\n      }\n    }\n  }\n  return !more\n}\n\n/**\n * Tokenize embedded tokens.\n *\n * @param {Array<Event>} events\n * @param {number} eventIndex\n * @returns {Record<string, number>}\n */\nfunction subcontent(events, eventIndex) {\n  const token = events[eventIndex][1]\n  const context = events[eventIndex][2]\n  let startPosition = eventIndex - 1\n  /** @type {Array<number>} */\n  const startPositions = []\n  const tokenizer =\n    token._tokenizer || context.parser[token.contentType](token.start)\n  const childEvents = tokenizer.events\n  /** @type {Array<[number, number]>} */\n  const jumps = []\n  /** @type {Record<string, number>} */\n  const gaps = {}\n  /** @type {Array<Chunk>} */\n  let stream\n  /** @type {Token | undefined} */\n  let previous\n  let index = -1\n  /** @type {Token | undefined} */\n  let current = token\n  let adjust = 0\n  let start = 0\n  const breaks = [start]\n\n  // Loop forward through the linked tokens to pass them in order to the\n  // subtokenizer.\n  while (current) {\n    // Find the position of the event for this token.\n    while (events[++startPosition][1] !== current) {\n      // Empty.\n    }\n    startPositions.push(startPosition)\n    if (!current._tokenizer) {\n      stream = context.sliceStream(current)\n      if (!current.next) {\n        stream.push(null)\n      }\n      if (previous) {\n        tokenizer.defineSkip(current.start)\n      }\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = true\n      }\n      tokenizer.write(stream)\n      if (current._isInFirstContentOfListItem) {\n        tokenizer._gfmTasklistFirstContentOfListItem = undefined\n      }\n    }\n\n    // Unravel the next token.\n    previous = current\n    current = current.next\n  }\n\n  // Now, loop back through all events (and linked tokens), to figure out which\n  // parts belong where.\n  current = token\n  while (++index < childEvents.length) {\n    if (\n      // Find a void token that includes a break.\n      childEvents[index][0] === 'exit' &&\n      childEvents[index - 1][0] === 'enter' &&\n      childEvents[index][1].type === childEvents[index - 1][1].type &&\n      childEvents[index][1].start.line !== childEvents[index][1].end.line\n    ) {\n      start = index + 1\n      breaks.push(start)\n      // Help GC.\n      current._tokenizer = undefined\n      current.previous = undefined\n      current = current.next\n    }\n  }\n\n  // Help GC.\n  tokenizer.events = []\n\n  // If there’s one more token (which is the cases for lines that end in an\n  // EOF), that’s perfect: the last point we found starts it.\n  // If there isn’t then make sure any remaining content is added to it.\n  if (current) {\n    // Help GC.\n    current._tokenizer = undefined\n    current.previous = undefined\n  } else {\n    breaks.pop()\n  }\n\n  // Now splice the events from the subtokenizer into the current events,\n  // moving back to front so that splice indices aren’t affected.\n  index = breaks.length\n  while (index--) {\n    const slice = childEvents.slice(breaks[index], breaks[index + 1])\n    const start = startPositions.pop()\n    jumps.unshift([start, start + slice.length - 1])\n    splice(events, start, 2, slice)\n  }\n  index = -1\n  while (++index < jumps.length) {\n    gaps[adjust + jumps[index][0]] = adjust + jumps[index][1]\n    adjust += jumps[index][1] - jumps[index][0] - 1\n  }\n  return gaps\n}\n"],"names":["subtokenize","events","jumps","index","event","lineIndex","otherIndex","otherEvent","parameters","subevents","more","subcontent","splice","eventIndex","token","context","startPosition","startPositions","tokenizer","childEvents","gaps","stream","previous","current","adjust","start","breaks","slice"],"mappings":"yDAeO,SAASA,EAAYC,EAAQ,CAElC,MAAMC,EAAQ,CAAA,EACd,IAAIC,EAAQ,GAERC,EAEAC,EAEAC,EAEAC,EAEAC,EAEAC,EAEAC,EACJ,KAAO,EAAEP,EAAQF,EAAO,QAAQ,CAC9B,KAAOE,KAASD,GACdC,EAAQD,EAAMC,CAAK,EAMrB,GAJAC,EAAQH,EAAOE,CAAK,EAKlBA,GACAC,EAAM,CAAC,EAAE,OAAS,aAClBH,EAAOE,EAAQ,CAAC,EAAE,CAAC,EAAE,OAAS,mBAE9BM,EAAYL,EAAM,CAAC,EAAE,WAAW,OAChCE,EAAa,EAEXA,EAAaG,EAAU,QACvBA,EAAUH,CAAU,EAAE,CAAC,EAAE,OAAS,oBAElCA,GAAc,GAGdA,EAAaG,EAAU,QACvBA,EAAUH,CAAU,EAAE,CAAC,EAAE,OAAS,WAElC,KAAO,EAAEA,EAAaG,EAAU,QAC1BA,EAAUH,CAAU,EAAE,CAAC,EAAE,OAAS,WAGlCG,EAAUH,CAAU,EAAE,CAAC,EAAE,OAAS,cACpCG,EAAUH,CAAU,EAAE,CAAC,EAAE,4BAA8B,GACvDA,KAOR,GAAIF,EAAM,CAAC,IAAM,QACXA,EAAM,CAAC,EAAE,cACX,OAAO,OAAOF,EAAOS,EAAWV,EAAQE,CAAK,CAAC,EAC9CA,EAAQD,EAAMC,CAAK,EACnBO,EAAO,YAIFN,EAAM,CAAC,EAAE,WAAY,CAG5B,IAFAE,EAAaH,EACbE,EAAY,OACLC,MACLC,EAAaN,EAAOK,CAAU,EAE5BC,EAAW,CAAC,EAAE,OAAS,cACvBA,EAAW,CAAC,EAAE,OAAS,oBAEnBA,EAAW,CAAC,IAAM,UAChBF,IACFJ,EAAOI,CAAS,EAAE,CAAC,EAAE,KAAO,mBAE9BE,EAAW,CAAC,EAAE,KAAO,aACrBF,EAAYC,GAMdD,IAEFD,EAAM,CAAC,EAAE,IAAM,OAAO,OAAO,CAAA,EAAIH,EAAOI,CAAS,EAAE,CAAC,EAAE,KAAK,EAG3DG,EAAaP,EAAO,MAAMI,EAAWF,CAAK,EAC1CK,EAAW,QAAQJ,CAAK,EACxBQ,EAAOX,EAAQI,EAAWF,EAAQE,EAAY,EAAGG,CAAU,EAE/D,CACF,CACA,MAAO,CAACE,CACV,CASA,SAASC,EAAWV,EAAQY,EAAY,CACtC,MAAMC,EAAQb,EAAOY,CAAU,EAAE,CAAC,EAC5BE,EAAUd,EAAOY,CAAU,EAAE,CAAC,EACpC,IAAIG,EAAgBH,EAAa,EAEjC,MAAMI,EAAiB,CAAA,EACjBC,EACJJ,EAAM,YAAcC,EAAQ,OAAOD,EAAM,WAAW,EAAEA,EAAM,KAAK,EAC7DK,EAAcD,EAAU,OAExBhB,EAAQ,CAAA,EAERkB,EAAO,CAAA,EAEb,IAAIC,EAEAC,EACAnB,EAAQ,GAERoB,EAAUT,EACVU,EAAS,EACTC,EAAQ,EACZ,MAAMC,EAAS,CAACD,CAAK,EAIrB,KAAOF,GAAS,CAEd,KAAOtB,EAAO,EAAEe,CAAa,EAAE,CAAC,IAAMO,GAAS,CAG/CN,EAAe,KAAKD,CAAa,EAC5BO,EAAQ,aACXF,EAASN,EAAQ,YAAYQ,CAAO,EAC/BA,EAAQ,MACXF,EAAO,KAAK,IAAI,EAEdC,GACFJ,EAAU,WAAWK,EAAQ,KAAK,EAEhCA,EAAQ,8BACVL,EAAU,mCAAqC,IAEjDA,EAAU,MAAMG,CAAM,EAClBE,EAAQ,8BACVL,EAAU,mCAAqC,SAKnDI,EAAWC,EACXA,EAAUA,EAAQ,IACpB,CAKA,IADAA,EAAUT,EACH,EAAEX,EAAQgB,EAAY,QAGzBA,EAAYhB,CAAK,EAAE,CAAC,IAAM,QAC1BgB,EAAYhB,EAAQ,CAAC,EAAE,CAAC,IAAM,SAC9BgB,EAAYhB,CAAK,EAAE,CAAC,EAAE,OAASgB,EAAYhB,EAAQ,CAAC,EAAE,CAAC,EAAE,MACzDgB,EAAYhB,CAAK,EAAE,CAAC,EAAE,MAAM,OAASgB,EAAYhB,CAAK,EAAE,CAAC,EAAE,IAAI,OAE/DsB,EAAQtB,EAAQ,EAChBuB,EAAO,KAAKD,CAAK,EAEjBF,EAAQ,WAAa,OACrBA,EAAQ,SAAW,OACnBA,EAAUA,EAAQ,MAqBtB,IAhBAL,EAAU,OAAS,CAAA,EAKfK,GAEFA,EAAQ,WAAa,OACrBA,EAAQ,SAAW,QAEnBG,EAAO,IAAG,EAKZvB,EAAQuB,EAAO,OACRvB,KAAS,CACd,MAAMwB,EAAQR,EAAY,MAAMO,EAAOvB,CAAK,EAAGuB,EAAOvB,EAAQ,CAAC,CAAC,EAC1DsB,EAAQR,EAAe,IAAG,EAChCf,EAAM,QAAQ,CAACuB,EAAOA,EAAQE,EAAM,OAAS,CAAC,CAAC,EAC/Cf,EAAOX,EAAQwB,EAAO,EAAGE,CAAK,CAChC,CAEA,IADAxB,EAAQ,GACD,EAAEA,EAAQD,EAAM,QACrBkB,EAAKI,EAAStB,EAAMC,CAAK,EAAE,CAAC,CAAC,EAAIqB,EAAStB,EAAMC,CAAK,EAAE,CAAC,EACxDqB,GAAUtB,EAAMC,CAAK,EAAE,CAAC,EAAID,EAAMC,CAAK,EAAE,CAAC,EAAI,EAEhD,OAAOiB,CACT","x_google_ignoreList":[0]}